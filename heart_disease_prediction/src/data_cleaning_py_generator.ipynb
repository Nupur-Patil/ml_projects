{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data_cleaning.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile data_cleaning.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "TARGET_COLUMN = ['num']\n",
    "NUMERICAL_FEATURES = ['age', 'trestbps', 'chol', 'thalch', 'oldpeak', 'ca']\n",
    "BINARY_FEATURES = ['sex', 'fbs', 'exang']\n",
    "OHE_FEATURES = ['cp', 'restecg', 'slope', 'thal']\n",
    "FEATURES = NUMERICAL_FEATURES + BINARY_FEATURES + OHE_FEATURES\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"Loads data from a CSV file.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        # Ensure all necessary columns are present, handling potential missing columns later\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Data file not found at {file_path}\")\n",
    "        return None\n",
    "    \n",
    "def impute_group_mode(series):\n",
    "    \"\"\"Imputes NA with mode of the categorical series.\"\"\"\n",
    "    mode_value = series.mode()\n",
    "    if mode_value.empty:\n",
    "        return series.fillna('MISSING_GROUP_MODE')\n",
    "    return series.fillna(mode_value.iloc[0])\n",
    "\n",
    "def impute_categoricals(df):\n",
    "    \"\"\"Creates imputation requests on relevant columns.\"\"\"\n",
    "    df['chol'] = df['chol'].replace(0, np.nan)\n",
    "    cat_cols_to_impute1 = ['fbs', 'restecg', 'exang']\n",
    "    cat_cols_to_impute2 = ['slope', 'thal']\n",
    "    for col in cat_cols_to_impute1:\n",
    "        df[col] = df.groupby(['age', 'sex'])[col].transform(impute_group_mode)\n",
    "    for col in cat_cols_to_impute2:\n",
    "        df[col] = df.groupby(['sex'])[col].transform(impute_group_mode)\n",
    "    return df\n",
    "\n",
    "    \n",
    "def create_preprocessing_pipeline():\n",
    "    \"\"\"\n",
    "    Creates the scikit-learn ColumnTransformer for all data preprocessing steps.\n",
    "    \n",
    "    Includes KNNImputer for numerical features, OneHot Encoding for categorical features \n",
    "    and StandardScaler.\n",
    "    \"\"\"\n",
    "    # 1. Handle binary features by replacing values with 1/0\n",
    "    df['sex'] = np.where(df['sex'] == 'Male', 1, 0)\n",
    "    df['fbs'] = np.where(df['fbs'] == True, 1, 0)\n",
    "    df['exang'] = np.where(df['exang'] == True, 1, 0)\n",
    "    \n",
    "    # 2. Numerical Pipeline: Impute missing values then scale\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        # Use KNNImputer to fill NaNs, converting to array to avoid feature name warning\n",
    "        ('imputer', KNNImputer(n_neighbors=5)), \n",
    "        # Standardize features (mean=0, variance=1)\n",
    "        ('scaler', StandardScaler()) \n",
    "    ])\n",
    "    \n",
    "    # 3. Categorical Pipeline: One-Hot Encode categorical features\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        # Handle 'unknown' categories and ensure consistent column names\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore')) \n",
    "    ])\n",
    "    \n",
    "    # 3. Column Transformer: Apply pipelines to the correct columns\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, NUMERICAL_FEATURES),\n",
    "            ('cat', categorical_transformer, OHE_FEATURES)\n",
    "        ],\n",
    "        remainder='passthrough',  # Keep any other columns if they exist\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    return preprocessor\n",
    "    \n",
    "def get_processed_data(df, test_size=0.25, random_state=42):\n",
    "    \"\"\"\n",
    "    Splits data and applies the preprocessing pipeline structure.\n",
    "    Returns: X_train, X_test, y_train, y_test, preprocessor\n",
    "    \"\"\"\n",
    "    X = df[FEATURES]\n",
    "    y = df[TARGET_COLUMN]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    preprocessor = create_preprocessing_pipeline()\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, preprocessor\n",
    "\n",
    "def save_data(df,file_path):\n",
    "    \"\"\"Saves preprocessed data to a CSV file.\"\"\"\n",
    "    try:\n",
    "        df = df[FEATURES + TARGET_COLUMN]\n",
    "        df.to_csv(file_path, index=False)\n",
    "        # Ensure all necessary columns are present, handling potential missing columns later\n",
    "        return None\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error in saving file at {file_path}\")\n",
    "        return None\n",
    "if __name__ == '__main__':\n",
    "    # This block runs if you execute this file directly\n",
    "    path_to_dataset = r'C:\\\\Users\\\\patil\\\\Documents\\\\GitHub\\\\ml_projects\\\\heart_disease_prediction\\\\data\\\\raw\\datasets\\redwankarimsony\\heart-disease-data\\versions\\6'\n",
    "    filename = 'heart_disease_uci.csv'\n",
    "    full_file_path = os.path.join(path_to_dataset, filename)\n",
    "    \n",
    "    df = load_data(full_file_path)\n",
    "    if df is None:\n",
    "        print(f'Error reading file')\n",
    "    else:\n",
    "        print('Data imported')\n",
    "    try:\n",
    "        imputed_df = impute_categoricals(df)\n",
    "    except:\n",
    "        print('Imputation error')\n",
    "        \n",
    "    X_train, X_test, y_train, y_test, preprocessor = get_processed_data(df)\n",
    "    \n",
    "    folder_path = r'C:\\\\Users\\\\patil\\\\Documents\\\\GitHub\\\\ml_projects\\\\heart_disease_prediction\\\\data\\\\processed'\n",
    "    filename = 'processed_data_py.csv'\n",
    "    full_file_path = os.path.join(folder_path, filename)\n",
    "    save_data(df,full_file_path)\n",
    "    \n",
    "    print(\"\\n--- Data Processor Check ---\")\n",
    "    print(f\"Training set size: {X_train.shape[0]} samples {X_train.shape[1]} columns\")\n",
    "    print(f\"Test set size: {X_test.shape[0]} samples {X_test.shape[1]} columns\")\n",
    "    print(\"\\nPreprocessing Pipeline created successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
